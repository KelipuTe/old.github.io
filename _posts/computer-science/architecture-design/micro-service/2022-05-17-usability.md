---
title: "微服务可用性（usability）"
date: 2022-05-29 08:00:00 +0800
tags: computer-science architecture-design micro-service
comment: false
show_author_profile: true
show_subscribe: false
---

- 隔离
  - 逻辑
    - 动静、读写、快慢、核心非核心、热点非热点
  - 物理
    - 线程、进程、集群、机房
- 超时控制
  - 进程内
    - 结构、算法
  - 进程间
    - 单机进程间
    - 网络间进程间
      - 数据库（连接超时、读超时、写超时）
  - 双峰分布
- 过载保护（load shedding）和限流
  - 限流依据
    - 系统指标
    - 滑动平均值
  - 限流方式
    - 令牌桶、漏桶、QPS
    - 优先级
      - 服务、接口
      - 用户
        - 识别异常用户
  - 自适应限流
    - 系统指标
      - 滑动平均值
    - 利特尔法则
  - 分布式限流
    - 限流中心（限流器）
    - 配额（quota）
      - 最大最小公平分享
  - 根据优先级限流
  - 熔断
    - 熔断器（断路器、Circuit Breakers）
    - 上游保护下游、客户端保护服务端
- 降级
- 重试
  - 次数、周期、只在失败处重试
- 负载均衡
  - 流量分发
    - P2C
  - 异常节点识别、同质节点扩容

### 隔离

#### 逻辑

动静隔离：动态数据和静态数据分开。

缓存（加速）变换频次小的数据（静态数据）。

- CDN 加速（静态数据）
- 数据库冷（静态数据）热表：数据更新的时候会刷新数据库缓存，冷热分离可以避免冷数据的缓存频繁被刷。

读写隔离：读操作和写操作分开。

读操作不会修改数据，可以缓存（加速）。

- 数据库读写分离
- CQRS 模式：查询操作（读操作）和命令操作

快慢隔离：不耗时的操作（快）和耗时的操作（慢）分开。

使用异步结构，防止耗时的操作堵住整个流程。

- 业务逻辑（快）和记录日志（慢）分开。

核心非核心：核心资源和非核心资源分开。

- 核心服务独享服务器；核心用户独享线路。

热点非热点：热点数据和非热点数据分开。

- 热点数据从缓存系统提升为本地缓存，防止对热点数据的访问量超过缓存系统的极限。

#### 物理

线程：不同的任务使用不同的线程（线程池）

- 业务逻辑（主线程执行）、消息通知（多线程异步处理）、记录日志（多线程异步处理）

进程：不同的服务放到不同的进程中执行

- 不同的服务单独部署（单机器、多机器）
- 不同的服务部署在不同的容器中（docker）
- 控制每个进程的资源（在 Linux 中，使用 cgroup）

集群：在多集群的基础上，不同类别的流量访问不同的集群

机房：在多机房（异地多活）的基础上，不同类别的流量访问不同的机房

### 超时控制

超时控制的目的是让某一个操作能够快速失效（fail fast），

#### 进程内

不合理的结构（逻辑结构和数据结构）或者不合理的算法造成执行耗时很长。

#### 进程间

主要是单机进程间通信超时和网络间进程间通信超时。

对于数据库来说网络间进程间通信超时还可以分为连接超时（网络不稳定等）、读超时（查询没有索引等）、写超时（插入和更新被锁等）。

可以通过超时传递

#### 双峰分布

95%（99%）的请求的耗时是在合理范围内的，5%（1%）的请求可能永远不会完成。超时控制需要关注的对象就是剩下的那 5% 或者 1% 的高耗时请求，这些请求占用资源是导致服务崩溃的关键。

### 过载保护和限流

过载保护的目的：

- 在系统稳定的前提下，最大限度保持系统的吞吐量。
- 在系统临近过载时，主动限流（抛弃请求），以求自保。

需要注意的是，抛弃（拒绝）请求也是需要成本的，有过载保护系统依然有可能被大量请求的网络连接打崩溃。

#### 自适应限流

在系统临近负载时（CPU 达到 90%、内存达到 90% 等），使用利特尔法则测量系统理论最大流量的滑动平均值，然后依次为基准进行限流。

利特尔法则：$L = λ \times W$，其中 L 为流量；λ 为流速；W 为时间跨度。$600ml = 10 ml/s * 60s$ 表示 $60 秒的总水流量 = 10 毫升每秒 * 60 秒$。

$系统每秒理论最大流量 = 每秒查询率（QPS） \time·s 请求平均响应时间$，三个值都是滑动平均值。

#### 分布式限流

简单的实现：每来一个请求，redis +1。

复杂一点的实现：每次心跳，从限流中心（限流器）批量申请配额（quota）。本地可以统计 QPS 的滑动平均值来决定每次申请多少配额。拿到配额后本地用令牌桶。

分配配额的方式：最大最小公平分享（Max-Min Fairness）

- 资源按照需求递增的顺序进行分配。
- 不存在用户得到的资源超过自己的需求。
- 未得到满足的用户等价的分享资源。

底层组件（数据库等）一定要做分布式限流。

#### 根据优先级限流

根据优先级限流可以和自适应限流和分布式限流配合使用。

- 达到某个负载时，先丢弃低优先级的请求（自适应限流）。
- 分配配额时，高优先级可以优先满足（分布式限流）。

#### 熔断

熔断是对下游的保护。如果熔断器（断路器、Circuit Breakers）发现下游服务大量报错，就不继续请求下游，而是直接返回定义好的错误。因为下游拒绝请求也需要消耗资源，如果上游因为失败不停的重试，有雪球效应可能会让下游过载。

当下游采用双机热备的成本和效益不匹配时，可以额外搭建一个小集群，承接主集群熔断时，多余的流量。当主机群恢复后，再切回去。

客户端需要对用户的积极的重试进行控制。

### 降级

降级本质为提供有损服务。

### 重试

- 重试次数：限制重试次数，防止请求一直占用资源
- 重试周期：随机化、指数型递增，防止重试请求叠出峰来
- 只在失败处重试：底层重试过了，上层就不要重试了，避免级联重试

### 负载均衡

P2C 算法：随机选取的两个节点进行打分，选择更优的节点。

- 对新启动的节点使用常量惩罚（penalty），然后使用探针方式的最小化放量进行预热。
- 打分比较低的节点，使用统计衰减的方式，让节点指标逐渐恢复到初始状态（即默认值），避免永久进入黑名单无法恢复。
- 如果发出去的请求超过了预测的延迟时间（predict latency），就会加惩罚。预测的延迟时间可以使用系统指标的滑动平均值配合统计衰减的方式。